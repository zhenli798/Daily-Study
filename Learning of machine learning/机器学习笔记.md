# 机器学习学习笔记

## 1. 何谓机器学习?

答：简单的说，机器学习就是把无序的数据转换成有用的信息。

## 2. 监督学习和非监督学习

1. **监督学习**：分类和回归
2. **非监督学习**：聚类和密度估计(数据没有类别信息，也不会给定目标值)

## 3. 分类算法

### 3.1 k-近邻算法(kNN)

1. **概述**：采用测量不同特征值之间的距离方法进行分类

2. **优点**：精度高、对异常值不敏感、无数据输入假定

3. **缺点**：计算复杂度高、空间复杂度高

4. **适用数据范围**：数值型和标称型

5. **工作原理**：存在一个样本数据集合，也称作样本训练集，且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。当输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后取样本集中特征最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，通常k是不大于20的整数，然后选择k个最相似的数据中出现次数最多的分类作为新数据的分类。

6. **一般流程**：
   1. 收集数据：可以使用任何方法
   2. 准备数据：距离计算所需要的数值，最好是结构化的数据格式
   3. 分析数据：可以使用任何方法
   4. 训练算法：此步骤不适用于k-近邻算法
   5. 测试算法：计算错误率
   6. 使用算法：首先需要输入样本数据和结构化输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。
   
7. **拓展**：

   * 归一化数值：将取值范围处理为0到1或-1到1之间，保证特征之间重要性是相等的。

8. **总结**：

   >k-近邻算法是基于实例学习的，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部的数据集，如果训练数据集很大，则势必会占用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。

### 3.2 决策树

1. **主要优势**：k-近邻算法的最大缺点就是无法给出数据的内在含义，决策树的主要优势就在于<font color = red>数据形式非常容易理解</font>。
2. **优点**：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
3. **缺点**：可能会产生过度匹配问题
4. **适用数据类型**：数值型和标称型
5. **一般流程**：
   1. 收集数据：可以使用任何方法
   2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
   3. 分析数据：可以使用任何方法，构造树完成只会，我们应该检查图形是否符合预期
   4. 训练算法：构造树的数据结构
   5. 测试算法：使用经验树计算错误率
   6. 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义